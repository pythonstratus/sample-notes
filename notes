from pyspark.sql import SparkSession
from tabulate import tabulate

def get_table_inventory():
    spark = SparkSession.builder.getOrCreate()
    inventory_data = []
    
    try:
        catalogs = spark.sql("SHOW CATALOGS").collect()
        
        for catalog in catalogs:
            catalog_name = catalog.catalog
            spark.sql(f"USE CATALOG {catalog_name}")
            schemas = spark.sql("SHOW SCHEMAS").collect()
            
            for schema in schemas:
                schema_name = schema.databaseName
                tables = spark.sql(f"SHOW TABLES IN {catalog_name}.{schema_name}").collect()
                
                for table in tables:
                    table_name = table.tableName
                    details = spark.sql(f"DESCRIBE DETAIL {catalog_name}.{schema_name}.{table_name}").collect()[0]
                    
                    inventory_data.append([
                        catalog_name,
                        schema_name,
                        table_name,
                        'external' if details.tableType == 'EXTERNAL' else 'managed',
                        details.format,
                        details.location
                    ])
        
        # Print formatted table
        headers = ['Catalog', 'Schema', 'Table Name', 'Type', 'Format', 'Location']
        print(tabulate(inventory_data, headers=headers, tablefmt='grid'))
        
    except Exception as e:
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    get_table_inventory()
