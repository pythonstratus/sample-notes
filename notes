# Read directly from S3 bucket using service account
df = spark.read.format("s3").option("recursiveFileLookup", "true").load("s3://bucket-name/path")

# Restore specific file types
parquet_df = spark.read.parquet("s3://bucket-name/path/*.parquet")
csv_df = spark.read.csv("s3://bucket-name/path/*.csv")

# Restore with specific options
df = spark.read.format("s3") \
  .option("mergeSchema", "true") \
  .option("pathGlobFilter", "*.csv") \
  .load("s3://bucket-name/path")

# Write restored data
df.write.mode("overwrite").saveAsTable("restored_table")
df.write.parquet("dbfs:/mnt/new-location/")

# Using dbutils for S3 operations
dbutils.fs.cp("s3://bucket-name/file.parquet", "dbfs:/mnt/destination/", recurse=True)
