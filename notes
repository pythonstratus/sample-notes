# Databricks notebook source
from pyspark.sql.functions import *
from pyspark.sql.types import *
import xml.etree.ElementTree as ET

# COMMAND ----------
# Define schema (keeping previous schema definition)
contract_schema = StructType([
    # ... previous schema fields remain the same ...
])

# COMMAND ----------
# Read XML file using spark.read instead of sparkContext
xml_df = spark.read.format("text").load("/Volumes/tagging/fpds_raw/landing/fpds_output_2025-01-03.xml")

# Get the XML content as string
xml_content = xml_df.collect()[0][0]

# COMMAND ----------
# Rest of the parsing logic remains the same
def parse_fpds_xml(xml_content):
    root = ET.fromstring(xml_content)
    # ... rest of parsing function remains same ...

# COMMAND ----------
# Process the data
contract_data = parse_fpds_xml(xml_content)
contract_df = spark.createDataFrame(contract_data, schema=contract_schema)

# COMMAND ----------
display(contract_df)
