from pyspark.sql import SparkSession

def get_table_inventory():
    spark = SparkSession.builder.getOrCreate()
    
    try:
        catalogs = spark.sql("SHOW CATALOGS").collect()
        
        # Print header
        print("\n{:<20} {:<20} {:<30} {:<15} {:<15} {:<50}".format(
            "Catalog", "Schema", "Table Name", "Type", "Format", "Location"))
        print("-" * 150)
        
        for catalog in catalogs:
            catalog_name = catalog.catalog
            spark.sql(f"USE CATALOG {catalog_name}")
            schemas = spark.sql("SHOW SCHEMAS").collect()
            
            for schema in schemas:
                schema_name = schema.databaseName
                tables = spark.sql(f"SHOW TABLES IN {catalog_name}.{schema_name}").collect()
                
                for table in tables:
                    table_name = table.tableName
                    details = spark.sql(f"DESCRIBE DETAIL {catalog_name}.{schema_name}.{table_name}").collect()[0]
                    
                    # Print formatted row
                    print("{:<20} {:<20} {:<30} {:<15} {:<15} {:<50}".format(
                        catalog_name,
                        schema_name,
                        table_name,
                        'external' if details.tableType == 'EXTERNAL' else 'managed',
                        details.format,
                        details.location
                    ))
                    
    except Exception as e:
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    get_table_inventory()
