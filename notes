from pyspark.sql.functions import current_timestamp, lit, count
from datetime import datetime

# Get metadata information
file_name = "fpds_output_2025-01-03.xml"
file_path = "/Volumes/staging/fpds_raw/landing/fpds_output_2025-01-03.xml"
num_rows = processed_df_3.count()
num_columns = len(processed_df_3.columns)
process_date = current_timestamp()
file_type = "XML"
file_process_state = "COMPLETED"

# Create metadata dataframe
metadata_df = spark.createDataFrame([
    (
        file_name,
        file_path,
        num_rows,
        num_columns,
        process_date,
        file_type,
        file_process_state
    )
], [
    "file_name",
    "file_path",
    "number_of_rows_ingested",
    "number_of_columns",
    "process_date",
    "file_type",
    "file_process_state"
])

# Write metadata to table
metadata_df.write \
    .format("delta") \
    .mode("append") \
    .saveAsTable("source_metadata")

# Verify the metadata was written
verification_df = spark.sql("SELECT * FROM source_metadata ORDER BY process_date DESC")
display(verification_df)
