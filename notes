from pyspark.sql import SparkSession

def generate_data_dictionary():
    spark = SparkSession.builder.getOrCreate()
    
    try:
        col_widths = {
            'table': 25,
            'column': 25,
            'type': 20,
            'description': 40,
            'logic': 40,
            'source': 25,
            'quality': 30
        }
        
        header = "{:<{w[table]}} {:<{w[column]}} {:<{w[type]}} {:<{w[description]}} {:<{w[logic]}} {:<{w[source]}} {:<{w[quality]}}".format(
            "Table Name", "Column Name", "Data Type", "Description", "Business Logic", "Source Field", "Quality Rules",
            w=col_widths
        )
        
        print("\n" + "=" * (sum(col_widths.values()) + 7))
        print(header)
        print("=" * (sum(col_widths.values()) + 7))
        
        tables = spark.sql("SHOW TABLES FROM staging.bk_mpo").collect()
        
        for table in tables:
            table_name = table.tableName
            columns = spark.sql(f"DESCRIBE staging.bk_mpo.{table_name}").collect()
            
            for col in columns:
                if col.col_name and not col.col_name.startswith('#'):
                    description = "No description available"
                    
                    print("{:<{w[table]}} {:<{w[column]}} {:<{w[type]}} {:<{w[description]}} {:<{w[logic]}} {:<{w[source]}} {:<{w[quality]}}".format(
                        table_name,
                        col.col_name,
                        col.data_type,
                        description,
                        "N/A",
                        "N/A",
                        "N/A",
                        w=col_widths
                    ))
            
            print("-" * (sum(col_widths.values()) + 7))
                    
    except Exception as e:
        print(f"Error: {str(e)}")

if __name__ == "__main__":
    generate_data_dictionary()
