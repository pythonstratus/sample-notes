import psycopg2
from typing import List, Dict, Optional
import pandas as pd
from datetime import datetime

def analyze_tables_and_indexes(
    host: str,
    database: str,
    user: str,
    password: str,
    port: int = 5432,
    schema: str = 'public'
) -> Dict:
    """
    Analyzes both indexed and non-indexed tables for potential keys in a single pass.
    """
    conn = psycopg2.connect(
        host=host,
        database=database,
        user=user,
        password=password,
        port=port
    )
    
    # Query to get all tables, columns, and their indexes
    tables_query = """
    SELECT 
        t.relname AS table_name,
        a.attname AS column_name,
        format_type(a.atttypid, a.atttypmod) AS data_type,
        a.attnotnull AS not_null,
        col.character_maximum_length,
        col.numeric_precision,
        col.numeric_scale,
        col.is_updatable,
        col.column_default,
        (SELECT count(*) FROM pg_constraint WHERE conrelid = t.oid) AS constraint_count,
        i.indexname AS index_name,
        i.indexdef AS index_definition,
        CASE WHEN i.indexname IS NOT NULL THEN TRUE ELSE FALSE END as has_index,
        CASE WHEN ix.indisprimary IS TRUE THEN TRUE ELSE FALSE END as is_primary,
        CASE WHEN ix.indisunique IS TRUE THEN TRUE ELSE FALSE END as is_unique
    FROM 
        pg_class t
        JOIN pg_attribute a ON a.attrelid = t.oid
        JOIN pg_namespace n ON t.relnamespace = n.oid
        JOIN information_schema.columns col ON 
            col.table_schema = n.nspname AND 
            col.table_name = t.relname AND 
            col.column_name = a.attname
        LEFT JOIN pg_indexes i ON 
            i.schemaname = n.nspname AND 
            i.tablename = t.relname AND 
            i.indexdef LIKE '%%' || col.column_name || '%%'
        LEFT JOIN pg_index ix ON 
            ix.indexrelid = (
                SELECT oid FROM pg_class WHERE relname = i.indexname
                AND relnamespace = n.oid
            )
    WHERE 
        n.nspname = %s
        AND t.relkind = 'r'
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY 
        t.relname, a.attnum;
    """
    
    # Query to get column statistics
    stats_query = """
    SELECT 
        schemaname,
        tablename,
        attname AS column_name,
        null_frac,
        n_distinct,
        correlation
    FROM 
        pg_stats
    WHERE 
        schemaname = %s;
    """
    
    try:
        # Get all tables and their details
        df_tables = pd.read_sql_query(tables_query, conn, params=[schema])
        df_stats = pd.read_sql_query(stats_query, conn, params=[schema])
        
        # Separate tables with and without indexes
        tables_with_indexes = df_tables[df_tables['has_index']].copy()
        tables_without_indexes = df_tables[~df_tables['has_index']].copy()
        
        results = {
            'indexed_recommendations': [],
            'non_indexed_recommendations': [],
            'schema': schema
        }
        
        # Analyze tables with indexes
        if not tables_with_indexes.empty:
            for table_name in tables_with_indexes['table_name'].unique():
                table_data = tables_with_indexes[tables_with_indexes['table_name'] == table_name]
                
                # Analyze potential PKs
                pk_candidates = []
                for _, col in table_data.iterrows():
                    if not col['is_primary']:
                        stats = df_stats[
                            (df_stats['tablename'] == table_name) &
                            (df_stats['column_name'] == col['column_name'])
                        ].iloc[0] if not df_stats.empty else None
                        
                        pk_score = calculate_pk_score(col, stats)
                        if pk_score > 0.7:
                            pk_candidates.append({
                                'column_name': col['column_name'],
                                'data_type': col['data_type'],
                                'score': pk_score,
                                'index_name': col['index_name'],
                                'index_definition': col['index_definition'],
                                'reasoning': get_pk_reasoning(col)
                            })
                
                # Analyze potential FKs
                fk_candidates = []
                for _, col in table_data.iterrows():
                    if not col['is_primary']:
                        matching_cols = df_tables[
                            (df_tables['column_name'] == col['column_name']) &
                            (df_tables['data_type'] == col['data_type']) &
                            (df_tables['table_name'] != table_name)
                        ]
                        
                        if not matching_cols.empty:
                            fk_score = calculate_fk_score(col, matching_cols)
                            if fk_score > 0.6:
                                fk_candidates.append({
                                    'column_name': col['column_name'],
                                    'data_type': col['data_type'],
                                    'score': fk_score,
                                    'potential_references': matching_cols['table_name'].unique().tolist()
                                })
                
                if pk_candidates or fk_candidates:
                    results['indexed_recommendations'].append({
                        'table_name': table_name,
                        'pk_candidates': pk_candidates,
                        'fk_candidates': fk_candidates
                    })
        
        # Analyze tables without indexes
        if not tables_without_indexes.empty:
            for table_name in tables_without_indexes['table_name'].unique():
                table_cols = tables_without_indexes[tables_without_indexes['table_name'] == table_name]
                
                # Analyze potential PKs for non-indexed tables
                pk_candidates = []
                for _, col in table_cols.iterrows():
                    pk_score = calculate_non_indexed_pk_score(col)
                    if pk_score > 0.5:
                        pk_candidates.append({
                            'column_name': col['column_name'],
                            'data_type': col['data_type'],
                            'score': pk_score,
                            'reasoning': get_pk_reasoning(col)
                        })
                
                # Analyze potential FKs for non-indexed tables
                fk_candidates = []
                for _, col in table_cols.iterrows():
                    matching_cols = df_tables[
                        (df_tables['column_name'] == col['column_name']) &
                        (df_tables['data_type'] == col['data_type']) &
                        (df_tables['table_name'] != table_name)
                    ]
                    
                    if not matching_cols.empty:
                        fk_candidates.append({
                            'column_name': col['column_name'],
                            'data_type': col['data_type'],
                            'potential_references': matching_cols['table_name'].unique().tolist()
                        })
                
                results['non_indexed_recommendations'].append({
                    'table_name': table_name,
                    'pk_candidates': pk_candidates,
                    'fk_candidates': fk_candidates,
                    'recommendations': generate_table_recommendations(table_cols)
                })
        
        return results
        
    finally:
        conn.close()

def calculate_pk_score(column: pd.Series, stats: Optional[pd.Series] = None) -> float:
    """Calculate score for indexed column as PK candidate."""
    score = 0.0
    
    # Unique constraint is crucial
    if column['is_unique']:
        score += 0.4
    
    # NOT NULL constraint is important
    if column['not_null']:
        score += 0.2
    
    # Data type suitability
    if column['data_type'].lower() in ('integer', 'bigint', 'uuid'):
        score += 0.1
    
    # Check null fraction if statistics available
    if stats is not None and stats['null_frac'] == 0:
        score += 0.1
    
    # Check distinct values if statistics available
    if stats is not None and stats['n_distinct'] and stats['n_distinct'] < 0:
        score += 0.1  # Negative n_distinct means it's a fraction of rows
    
    # Existing constraints might indicate importance
    if column['constraint_count'] > 0:
        score += 0.1
    
    return min(1.0, score)

def calculate_non_indexed_pk_score(column: pd.Series) -> float:
    """Calculate score for non-indexed column as PK candidate."""
    score = 0.0
    
    # Data type suitability
    if column['data_type'].lower() in ('integer', 'bigint', 'uuid'):
        score += 0.3
    elif column['data_type'].lower() in ('character varying', 'text'):
        score += 0.1
    
    # NOT NULL constraint
    if column['not_null']:
        score += 0.3
    
    # Has default value (potential sequence/identity)
    if column['column_default'] is not None:
        score += 0.2
    
    # Column name suggests ID
    if 'id' in column['column_name'].lower():
        score += 0.1
    
    return min(1.0, score)

def calculate_fk_score(source: pd.Series, targets: pd.DataFrame) -> float:
    """Calculate score for foreign key candidate."""
    score = 0.0
    
    # Having an index is good for FK performance
    if source['has_index']:
        score += 0.3
    
    # Matching data types is crucial
    if not targets.empty:
        score += 0.3
        
        # Check if any target is a PK or unique
        if targets['is_primary'].any() or targets['is_unique'].any():
            score += 0.2
    
    # Not being nullable is good for referential integrity
    if source['not_null']:
        score += 0.2
    
    return score

def get_pk_reasoning(column: pd.Series) -> List[str]:
    """Generate explanation for PK suggestion."""
    reasons = []
    
    if column['not_null']:
        reasons.append("Column is NOT NULL")
    
    if column['data_type'].lower() in ('integer', 'bigint', 'uuid'):
        reasons.append("Suitable data type for PK")
    
    if column['column_default'] is not None:
        reasons.append("Has default value/sequence")
    
    if column.get('is_unique', False):
        reasons.append("Has unique constraint")
    
    if 'id' in column['column_name'].lower():
        reasons.append("Column name suggests identifier")
    
    return reasons

def generate_table_recommendations(table_cols: pd.DataFrame) -> List[str]:
    """Generate recommendations for tables without indexes."""
    recs = [
        "Consider adding indexes for frequently queried columns",
        "Review table usage patterns to identify optimal index candidates"
    ]
    
    # Check for timestamp/audit columns
    if not any('created' in col.lower() or 'updated' in col.lower() 
               for col in table_cols['column_name']):
        recs.append("Consider adding audit columns (created_at, updated_at)")
    
    return recs

def generate_enhanced_report(analysis_results: Dict) -> str:
    """Generate comprehensive analysis report."""
    report = []
    report.append(f"# Database Key Analysis Report - Schema: {analysis_results['schema']}\n")
    report.append(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Confidence Score Explanation
    report.append("## Confidence Score Explanation")
    report.append("Scores are calculated based on the following criteria:\n")
    report.append("### For Indexed Tables:")
    report.append("- Uniqueness (40%): Column has unique constraint")
    report.append("- NOT NULL constraint (20%): Column doesn't allow nulls")
    report.append("- Data Type (10%): Column is INTEGER, BIGINT, or UUID")
    report.append("- Null Fraction (10%): No null values in actual data")
    report.append("- Distinct Values (10%): High ratio of unique values")
    report.append("- Existing Constraints (10%): Column has other constraints")
    report.append("\nScores above 70% indicate good candidates for primary keys.\n")
    
    # Report on tables without indexes
    report.append("## Tables Without Indexes")
    if analysis_results['non_indexed_recommendations']:
        for table in analysis_results['non_indexed_recommendations']:
            report.append(f"\n### Table: {table['table_name']}")
            
            # PK candidates
            if table['pk_candidates']:
                report.append("\nPotential Primary Key Candidates:")
                for pk in table['pk_candidates']:
                    report.append(f"- Column: {pk['column_name']}")
                    report.append(f"  * Data Type: {pk['data_type']}")
                    report.append(f"  * Confidence Score: {pk['score']:.2%}")
                    report.append("  * Reasoning:")
                    for reason in pk['reasoning']:
                        report.append(f"    - {reason}")
            
            # FK candidates
            if table['fk_candidates']:
                report.append("\nPotential Foreign Key Candidates:")
                for fk in table['fk_candidates']:
                    report.append(f"- Column: {fk['column_name']}")
                    report.append(f"  * Data Type: {fk['data_type']}")
                    report.append(f"  * Could Reference: {', '.join(fk['potential_references'])}")
            
            # Recommendations
            if table['recommendations']:
                report.append("\nRecommendations:")
                for rec in table['recommendations']:
                    report.append(f"- {rec}")
    else:
        report.append("\nNo tables without indexes found.")
    
    # Report on tables with indexes
    report.append("\n## Tables With Indexes")
    if analysis_results['indexed_recommendations']:
        for table in analysis_results['indexed_recommendations']:
            report.append(f"\n### Table: {table['table_name']}")
            
            # PK candidates
            if table['pk_candidates']:
                report.append("\nPotential Primary Key Candidates:")
                for pk in table['pk_candidates']:
                    report.append(f"- Column: {pk['column_name']}")
                    report.append(f"  * Data Type: {pk['data_type']}")
                    report.append(f"  * Confidence Score: {pk['score']:.2%}")
                    report.append(f"  * Current Index: {pk['index_name']}")
                    report.append("  * Reasoning:")
                    for reason in pk['reasoning']:
                        report.append(f"    - {reason}")
            
            # FK candidates
            if table['fk_candidates']:
                report.append("\nPotential Foreign Key Candidates:")
                for fk in table['fk_candidates']:
                    report.append(f"- Column: {fk['column_name']}")
                    report.append(f"  * Data Type: {fk['data_type']}")
                    report.append(f"  * Score: {fk['score']:.2%}")
                    report.append(f"  * Could Reference: {', '.join(fk['potential_references'])}")
    else:
        report.append("\nNo potential key candidates found in indexed tables.")
    
    return "\n".join(report)

def save_report_to_file(report: str, schema: str) -> str:
    """Save the analysis report to a file."""
    filename = f"db_key_analysis_{schema}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report)
    return filename

if __name__ == "__main__":
    # Database connection parameters
    DB_PARAMS = {
        "host": "localhost",
        "database": "your_database",
        "user": "your_user",
        "password": "your_password",
        "port": 5432,
        "schema": "your_schema"
    }
    
    try:
        # Run analysis
        results = analyze_tables_and_indexes(**DB_PARAMS)
        
        # Generate and save report
        report = generate_enhanced_report(results)
        output_file = save_report_to_file(report, results['schema'])
        
        print(f"\nAnalysis complete! Report saved to: {output_file}")
        
        # Optional: Print report to console
        print("\nReport contents:")
        print(report)
        
    except Exception as e:
        print(f"Error during analysis: {str(e)}")
