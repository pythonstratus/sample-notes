from pyspark.sql import SparkSession
from datetime import datetime
from IPython.display import display, HTML
import pyspark.sql.functions as F

def generate_schema_readme(schema_name="bk_MPO", database="staging"):
    """
    Generate a comprehensive README dashboard for a Databricks schema
    """
    spark = SparkSession.builder.getOrCreate()
    
    def get_table_info():
        """Get information about all tables in the schema, excluding system tables"""
        try:
            # Check for tables under the schema
            tables = spark.sql(f"SHOW TABLES IN {database}.{schema_name}")
            
            # Return empty list if no tables found
            if tables.count() == 0:
                print(f"No tables found in {database}.{schema_name}")
                return []
            
            # Convert to list of tuples (name, description), excluding system tables (starting with _)
            return [(row.tableName, "") for row in tables.collect() if not row.tableName.startswith('_')]
        except Exception as e:
            print(f"Error accessing schema {database}.{schema_name}: {str(e)}")
            return []
    
    def get_column_statistics(df, col_name, data_type):
        """Get relevant statistics based on column data type"""
        try:
            stats = {}
            
            # For numeric columns
            if any(t in data_type.lower() for t in ['int', 'double', 'decimal', 'float']):
                stats_df = df.agg(
                    F.count(col_name).alias('count'),
                    F.count(F.when(F.col(col_name).isNull(), True)).alias('null_count'),
                    F.min(col_name).alias('min'),
                    F.max(col_name).alias('max')
                ).collect()[0]
                
                total_count = stats_df['count']
                null_count = stats_df['null_count']
                
                stats['completeness'] = f"{((total_count - null_count) / total_count * 100):.1f}% populated"
                if null_count > 0:
                    stats['nulls'] = f"{null_count:,} nulls"
                stats['range'] = f"Range: {stats_df['min']} to {stats_df['max']}"
                
            # For string columns
            elif 'string' in data_type.lower():
                stats_df = df.agg(
                    F.count(col_name).alias('count'),
                    F.count(F.when(F.col(col_name).isNull(), True)).alias('null_count'),
                    F.countDistinct(col_name).alias('distinct_count')
                ).collect()[0]
                
                total_count = stats_df['count']
                null_count = stats_df['null_count']
                distinct_count = stats_df['distinct_count']
                
                stats['completeness'] = f"{((total_count - null_count) / total_count * 100):.1f}% populated"
                if null_count > 0:
                    stats['nulls'] = f"{null_count:,} nulls"
                stats['cardinality'] = f"{distinct_count:,} unique values"
                
            # For timestamp/date columns
            elif any(t in data_type.lower() for t in ['timestamp', 'date']):
                stats_df = df.agg(
                    F.count(col_name).alias('count'),
                    F.count(F.when(F.col(col_name).isNull(), True)).alias('null_count'),
                    F.min(col_name).alias('min_date'),
                    F.max(col_name).alias('max_date')
                ).collect()[0]
                
                total_count = stats_df['count']
                null_count = stats_df['null_count']
                
                stats['completeness'] = f"{((total_count - null_count) / total_count * 100):.1f}% populated"
                if null_count > 0:
                    stats['nulls'] = f"{null_count:,} nulls"
                stats['timespan'] = f"From {stats_df['min_date']} to {stats_df['max_date']}"
            
            return " | ".join(stats.values())
            
        except Exception as e:
            return f"Error calculating statistics: {str(e)}"
    
    def get_table_stats(table_name):
        """Get detailed statistics for a specific table"""
        full_table_name = f"{database}.{schema_name}.{table_name}"
        
        # Verify table exists before attempting to get statistics
        try:
            table_exists = spark.sql(f"SHOW TABLES IN {database}.{schema_name} LIKE '{table_name}'").count() > 0
            if not table_exists:
                raise Exception(f"Table {full_table_name} does not exist")
            
            df = spark.table(full_table_name)
        except Exception as e:
            raise Exception(f"Error accessing table {full_table_name}: {str(e)}")
        
        # Get basic table info
        row_count = df.count()
        columns = df.columns
        col_count = len(columns)
        
        # Get data types and statistics
        schema_info = {}
        for field in df.schema:
            col_name = field.name
            data_type = str(field.dataType)
            statistics = get_column_statistics(df, col_name, data_type)
            schema_info[col_name] = {
                'data_type': data_type,
                'statistics': statistics
            }
        
        # Get last modified time
        try:
            history_df = spark.sql(f"DESCRIBE HISTORY {full_table_name}")
            last_modified = history_df.select('timestamp').first()[0] if history_df.count() > 0 else "No history available"
        except Exception:
            last_modified = "History not available"
            
        return {
            "row_count": row_count,
            "column_count": col_count,
            "columns": schema_info,
            "last_modified": last_modified
        }
    
    # Generate HTML output
    tables = get_table_info()
    html_output = f"""
    <div style="padding: 20px;">
        <h1>{schema_name} Schema Documentation</h1>
        <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        
        <h2>Schema Overview</h2>
        <ul>
            <li>Total Tables: {len(tables)}</li>
            <li>Database: {database}</li>
            <li>Schema: {schema_name}</li>
        </ul>
        
        <h2>Table Details</h2>
    """
    
    # Generate detailed table information
    for table_name, description in tables:
        try:
            stats = get_table_stats(table_name)
            
            html_output += f"""
            <details>
                <summary style="cursor: pointer; padding: 10px; background-color: #f0f0f0; margin: 5px 0;">
                    <strong>{table_name}</strong> - {stats['row_count']:,} rows, {stats['column_count']} columns
                </summary>
                <div style="padding: 10px; margin-left: 20px;">
                    <p>Description: {description if description else 'No description available'}</p>
                    <p>Last Modified: {stats['last_modified']}</p>
                    
                    <h4>Columns:</h4>
                    <table style="width: 100%; border-collapse: collapse;">
                        <tr>
                            <th style="text-align: left; padding: 5px; border: 1px solid #ddd;">Column Name</th>
                            <th style="text-align: left; padding: 5px; border: 1px solid #ddd;">Data Type</th>
                            <th style="text-align: left; padding: 5px; border: 1px solid #ddd;">Statistics</th>
                        </tr>
            """
            
            for col_name, info in stats['columns'].items():
                html_output += f"""
                        <tr>
                            <td style="padding: 5px; border: 1px solid #ddd;">{col_name}</td>
                            <td style="padding: 5px; border: 1px solid #ddd;">{info['data_type']}</td>
                            <td style="padding: 5px; border: 1px solid #ddd;">{info['statistics']}</td>
                        </tr>
                """
                
            html_output += """
                    </table>
                </div>
            </details>
            """
        except Exception as e:
            html_output += f"""
            <details>
                <summary style="cursor: pointer; padding: 10px; background-color: #ffebee; margin: 5px 0;">
                    <strong>{table_name}</strong> - Error loading table details
                </summary>
                <div style="padding: 10px; margin-left: 20px;">
                    <p>Error: {str(e)}</p>
                </div>
            </details>
            """
    
    html_output += "</div>"
    display(HTML(html_output))

# Example usage
# generate_schema_readme("bk_MPO", "staging")



************************************************************************************
# Databricks Schema README Generator

## Overview
This tool generates an interactive HTML dashboard within Databricks notebooks that serves as a comprehensive documentation for your schemas and tables. It provides a self-updating, explorable interface for data discovery and documentation.

## Key Features

### 1. Schema-Level Information
- Total number of tables in the schema
- Database and schema identification
- Generation timestamp for freshness tracking

### 2. Table-Level Details
- Row counts and column counts for each table
- Last modification timestamps from table history
- Expandable/collapsible interface for each table
- Automatic exclusion of system tables (prefixed with '_')

### 3. Column-Level Analytics
Based on data type, the tool provides different relevant statistics:

#### Numeric Columns
- Completeness percentage
- Null value counts
- Value ranges (min/max)

#### String Columns
- Completeness percentage
- Null value counts
- Cardinality (unique value counts)

#### Date/Timestamp Columns
- Completeness percentage
- Null value counts
- Time span coverage (earliest to latest)

## Benefits of Using This README Generator

### 1. Data Discovery
- **Quick Overview**: Instantly understand schema structure and contents
- **Interactive Exploration**: Expand only tables of interest
- **Type-Aware Statistics**: Relevant metrics based on column types

### 2. Data Quality Monitoring
- **Completeness Tracking**: Monitor null values and data population
- **Range Validation**: Identify potential anomalies in value ranges
- **Cardinality Analysis**: Understand string column distributions

### 3. Documentation
- **Self-Updating**: Statistics always reflect current data state
- **Consistent Format**: Standardized documentation across schemas
- **Comprehensive Coverage**: All tables and columns documented

### 4. Compatibility
- Works with any Databricks schema
- Compatible with Delta and non-Delta tables
- Handles various data types appropriately
- Platform-agnostic within Databricks (SQL, Python, R notebooks)

### 5. Maintenance Benefits
- **Schema Evolution**: Automatically adapts to schema changes
- **No External Dependencies**: Uses only built-in Databricks features
- **Error Handling**: Graceful handling of access issues and missing tables

## Usage

### Basic Usage
```python
generate_schema_readme("your_schema_name", "your_database")
```

### Example Implementation
1. Create a new notebook named "Schema README"
2. Add the generator code
3. Schedule to run daily/weekly to keep documentation fresh
4. Pin the notebook for easy access

### Best Practices
1. Create README notebooks for each major schema
2. Schedule regular updates to maintain freshness
3. Use as part of data governance documentation
4. Reference in onboarding materials for new team members

## Integration Ideas

### 1. Data Governance
- Link to governance policies
- Track schema changes over time
- Document data ownership and stewardship

### 2. Data Quality Monitoring
- Set up alerts for unexpected changes
- Monitor data completeness trends
- Track cardinality changes

### 3. Development Workflows
- Include in CI/CD pipelines
- Automate documentation updates
- Track schema evolution

## Technical Notes

### Performance Considerations
- Uses efficient Spark SQL commands
- Lazy evaluation for statistics
- Handles large schemas gracefully
- Excludes system tables for efficiency

### Security Features
- Respects Databricks security model
- Shows only accessible tables
- Handles permission errors gracefully

## Future Enhancement Possibilities

1. **Additional Statistics**
   - Column correlations
   - Data distribution visualizations
   - Pattern matching for string columns

2. **Integration Features**
   - Links to sample queries
   - Usage statistics integration
   - Data lineage visualization

3. **Documentation Enhancements**
   - Custom metadata support
   - Business glossary integration
   - Change tracking visualization
