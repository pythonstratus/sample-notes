from pyspark.sql.functions import col, lit

def get_safe_columns(df, column_paths):
    available_columns = set(df.columns)
    safe_cols = []
    
    for col_path, alias_name in column_paths:
        try:
            if df.select(col_path).columns[0] in available_columns:
                safe_cols.append(col(col_path).cast("string").alias(alias_name))
            else:
                safe_cols.append(lit(None).cast("string").alias(alias_name))
        except:
            safe_cols.append(lit(None).cast("string").alias(alias_name))
    
    return safe_cols

column_paths = [
    ("ns0:content.ns1:award.ns1:placeOfPerformance.ns1:placeOfPerformanceCongressionalDistrict", "congressional_district"),
    ("ns0:content.ns1:award.ns1:contractData.ns1:contingencyHumanitarianPeacekeepingOperation._value", "peace_keeping_op"),
    ("ns0:content.ns1:award.ns1:productOrServiceInformation.ns1:contractBundling._value", "contract_bundling"),
    ("ns0:content.ns1:award.ns1:vendor.ns1:vendorSiteDetails.ns1:vendorOrganizationFactors.ns1:countryOfIncorporation", "country_of_incorporation"),
    ("ns0:content.ns1:award.ns1:relevantContractDates.ns1:currentCompletionDate", "completion_date"),
    ("ns0:content.ns1:award.ns1:competition.ns1:fedBizOpps._value", "fed_biz_opps"),
    ("ns0:content.ns1:award.ns1:contractMarketingData.ns1:feePaidForUseOfService", "fee_paid"),
    ("ns0:content.ns1:award.ns1:competition.ns1:idvNumberOfOffersReceived", "idv_number_of_offers"),
    ("ns0:content.ns1:award.ns1:productOrServiceInformation.ns1:informationTechnologyCommercialItemCategory", "it_commercial_category")
]

processed_df_2 = contract_df_2.select(get_safe_columns(contract_df_2, column_paths))
